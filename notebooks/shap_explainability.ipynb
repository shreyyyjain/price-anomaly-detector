{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8835f3-6cc9-4478-a1b7-4ee26417a5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies detected: 1217\n",
      "Anomaly data saved to C:\\Users\\shrey\\Desktop\\Projects\\Explainable Price Anomaly Detector for Indian Second-hand Marketplace\\reports\\anomalies.csv\n",
      "README.md updated with SHAP explainability summary.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = r'C:\\Users\\shrey\\Desktop\\Projects\\Explainable Price Anomaly Detector for Indian Second-hand Marketplace'\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'cleaned_engineered.csv')\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT, 'models', 'baseline_model.pkl')\n",
    "SCALER_PATH = os.path.join(PROJECT_ROOT, 'models', 'scaler.pkl')\n",
    "FEATURE_PATH = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "REPORTS_PATH = os.path.join(PROJECT_ROOT, 'reports')\n",
    "os.makedirs(REPORTS_PATH, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Log-transform target\n",
    "df['log_price'] = np.log1p(df['listed_price'])\n",
    "\n",
    "# Smoothed target encoding for high-cardinality categories\n",
    "mean_global = df['log_price'].mean()\n",
    "k = 5  # smoothing factor\n",
    "for col in ['oem', 'model', 'city']:\n",
    "    target_mean = df.groupby(col)['log_price'].mean()\n",
    "    count = df.groupby(col)['log_price'].count()\n",
    "    smooth = (target_mean * count + mean_global * k) / (count + k)\n",
    "    df[f'{col}_target_enc'] = df[col].map(smooth)\n",
    "\n",
    "# Frequency encoding for categorical variables\n",
    "for col in ['oem', 'model', 'city']:\n",
    "    freq = df[col].value_counts()\n",
    "    df[f'{col}_freq_enc'] = df[col].map(freq)\n",
    "\n",
    "# Interaction features\n",
    "df['brand_age'] = df['car_age'] * df['oem_target_enc']\n",
    "df['km_per_year_age'] = df['km_per_year'] * df['car_age']\n",
    "df['power_weight_ratio'] = df['max power delivered'] / df['kerb weight']\n",
    "\n",
    "# Load model, scaler, and feature names\n",
    "with open(MODEL_PATH, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "with open(SCALER_PATH, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "with open(FEATURE_PATH, 'rb') as f:\n",
    "    feature_names = pickle.load(f)\n",
    "\n",
    "# Define features (same as in feature_engineering_experiments.ipynb)\n",
    "num_cols = [\n",
    "    'km', 'car_age', 'km_per_year', 'max power delivered', 'alloy wheel size',\n",
    "    'length', 'width', 'height', 'wheel base', 'front tread', 'rear tread',\n",
    "    'kerb weight', 'gross weight', 'top speed', 'acceleration', 'bore',\n",
    "    'oem_target_enc', 'model_target_enc', 'city_target_enc',\n",
    "    'brand_age', 'km_per_year_age', 'power_weight_ratio'\n",
    "]\n",
    "cat_cols = [\n",
    "    'transmission', 'fuel', 'owner_type', 'drive type', 'steering type',\n",
    "    'front brake type', 'rear brake type', 'tyre type'\n",
    "]\n",
    "\n",
    "# Keep only existing columns\n",
    "num_cols = [col for col in num_cols if col in df.columns]\n",
    "cat_cols = [col for col in cat_cols if col in df.columns]\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[num_cols + cat_cols].copy()\n",
    "y = df['log_price']\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "for col in cat_cols:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "# Scale numerical features\n",
    "X[num_cols] = scaler.transform(X[num_cols])\n",
    "\n",
    "# Compute predictions\n",
    "y_pred = model.predict(X)\n",
    "y_pred_actual = np.expm1(y_pred)\n",
    "y_actual = np.expm1(y)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = np.abs(y_actual - y_pred_actual)\n",
    "\n",
    "# Flag anomalies (e.g., residuals > 2 standard deviations)\n",
    "residual_mean = residuals.mean()\n",
    "residual_std = residuals.std()\n",
    "anomaly_threshold = residual_mean + 2 * residual_std\n",
    "anomalies = residuals > anomaly_threshold\n",
    "print(f'Number of anomalies detected: {anomalies.sum()}')\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# Summary plot (feature importance)\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X, show=False)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.savefig(os.path.join(REPORTS_PATH, 'shap_summary_plot.png'))\n",
    "plt.close()\n",
    "\n",
    "# Dependence plot for top feature (e.g., 'width' from RandomForest importance)\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.dependence_plot('width', shap_values, X, show=False)\n",
    "plt.title('SHAP Dependence Plot for Width')\n",
    "plt.savefig(os.path.join(REPORTS_PATH, 'shap_dependence_width.png'))\n",
    "plt.close()\n",
    "\n",
    "# Force plot for a sample anomaly\n",
    "anomaly_idx = np.where(anomalies)[0][0]  # First anomaly\n",
    "plt.figure(figsize=(12, 4))\n",
    "shap.force_plot(explainer.expected_value, shap_values[anomaly_idx], X.iloc[anomaly_idx], matplotlib=True, show=False)\n",
    "plt.title(f'SHAP Force Plot for Anomaly (Index {anomaly_idx})')\n",
    "plt.savefig(os.path.join(REPORTS_PATH, f'shap_force_plot_anomaly_{anomaly_idx}.png'))\n",
    "plt.close()\n",
    "\n",
    "# Scatter plot of predicted vs. actual prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_actual[~anomalies], y_pred_actual[~anomalies], c='blue', alpha=0.5, label='Normal')\n",
    "plt.scatter(y_actual[anomalies], y_pred_actual[anomalies], c='red', alpha=0.5, label='Anomaly')\n",
    "plt.plot([y_actual.min(), y_actual.max()], [y_actual.min(), y_actual.max()], 'k--')\n",
    "plt.xlabel('Actual Price (₹)')\n",
    "plt.ylabel('Predicted Price (₹)')\n",
    "plt.title('Predicted vs. Actual Prices with Anomalies Highlighted')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.savefig(os.path.join(REPORTS_PATH, 'predicted_vs_actual.png'))\n",
    "plt.close()\n",
    "\n",
    "# Save anomaly data\n",
    "anomaly_df = df[anomalies].copy()\n",
    "anomaly_df['predicted_price'] = y_pred_actual[anomalies]\n",
    "anomaly_df['residual'] = residuals[anomalies]\n",
    "anomaly_df.to_csv(os.path.join(REPORTS_PATH, 'anomalies.csv'), index=False)\n",
    "print(f'Anomaly data saved to {os.path.join(REPORTS_PATH, \"anomalies.csv\")}')\n",
    "\n",
    "# Update README\n",
    "readme_content = f\"\"\"\n",
    "# SHAP Explainability Summary\n",
    "- Used SHAP to explain LightGBM model predictions.\n",
    "- Identified {anomalies.sum()} anomalies based on residuals > 2 standard deviations (threshold: ₹{anomaly_threshold:,.0f}).\n",
    "- Generated SHAP summary plot, dependence plot for 'width', force plot for a sample anomaly, and predicted vs. actual scatter plot.\n",
    "- Visuals saved in reports/: shap_summary_plot.png, shap_dependence_width.png, shap_force_plot_anomaly_{anomaly_idx}.png, predicted_vs_actual.png.\n",
    "- Anomaly data saved to reports/anomalies.csv.\n",
    "- Next steps: Implement business logic and rule-based checks for anomaly validation.\n",
    "\"\"\"\n",
    "with open(os.path.join(PROJECT_ROOT, 'README.md'), 'a', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "print('README.md updated with SHAP explainability summary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4099b10-3dfc-4646-a0c0-3fbad4e71c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
