{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696df713-4b38-4b00-9826-9ce136670d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 21:38:51.584 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-09-17 21:38:54.252 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = r'C:\\Users\\shrey\\Desktop\\Projects\\Explainable Price Anomaly Detector for Indian Second-hand Marketplace'\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, 'data', 'cleaned_engineered.csv')\n",
    "ANOMALIES_PATH = os.path.join(PROJECT_ROOT, 'reports', 'validated_anomalies.csv')\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT, 'models', 'baseline_model.pkl')\n",
    "SCALER_PATH = os.path.join(PROJECT_ROOT, 'models', 'scaler.pkl')\n",
    "FEATURE_PATH = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "REPORTS_PATH = os.path.join(PROJECT_ROOT, 'reports')\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Price Anomaly Detector for Indian Second-hand Marketplace\")\n",
    "st.markdown(\"Explore validated price anomalies with SHAP explanations.\")\n",
    "\n",
    "# Load validated anomalies\n",
    "try:\n",
    "    anomaly_df = pd.read_csv(ANOMALIES_PATH, low_memory=False)\n",
    "    # Reset index to simple integers and drop any existing index column\n",
    "    if anomaly_df.index.name is not None or 'index' in anomaly_df.columns:\n",
    "        anomaly_df = anomaly_df.reset_index(drop=True)\n",
    "    # Store original indices for SHAP\n",
    "    anomaly_df['original_index'] = anomaly_df.index\n",
    "    \n",
    "    # Convert rule_violations to clean string\n",
    "    def clean_rule_violations(x):\n",
    "        if isinstance(x, str) and x.startswith('['):\n",
    "            try:\n",
    "                return ', '.join(eval(x))\n",
    "            except:\n",
    "                return str(x)\n",
    "        return str(x)\n",
    "    \n",
    "    anomaly_df['rule_violations'] = anomaly_df['rule_violations'].apply(clean_rule_violations)\n",
    "    \n",
    "    # Convert columns to Arrow-compatible types\n",
    "    numeric_cols = ['listed_price', 'predicted_price', 'residual', 'km', 'car_age']\n",
    "    for col in numeric_cols:\n",
    "        if col in anomaly_df.columns:\n",
    "            anomaly_df[col] = pd.to_numeric(anomaly_df[col], errors='coerce').astype('float64').fillna(0)\n",
    "    \n",
    "    # Convert all other columns to string\n",
    "    for col in anomaly_df.columns:\n",
    "        if col not in numeric_cols + ['original_index']:\n",
    "            anomaly_df[col] = anomaly_df[col].astype(str).fillna('Unknown')\n",
    "    \n",
    "    # Debug types (uncomment if needed)\n",
    "    # st.write(\"Anomaly DataFrame types:\", anomaly_df.dtypes)\n",
    "    # st.write(\"Sample anomaly data:\", anomaly_df.head())\n",
    "    st.write(f\"**Total Validated Anomalies**: {len(anomaly_df)}\")\n",
    "except FileNotFoundError:\n",
    "    st.error(f\"File not found: {ANOMALIES_PATH}\")\n",
    "    st.stop()\n",
    "\n",
    "# Load full dataset\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "except FileNotFoundError:\n",
    "    st.error(f\"File not found: {DATA_PATH}\")\n",
    "    st.stop()\n",
    "\n",
    "# Load model, scaler, feature names\n",
    "try:\n",
    "    with open(MODEL_PATH, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(SCALER_PATH, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    with open(FEATURE_PATH, 'rb') as f:\n",
    "        feature_names = pickle.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    st.error(f\"Model or scaler file not found: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "# Feature engineering\n",
    "df['log_price'] = np.log1p(df['listed_price'])\n",
    "mean_global = df['log_price'].mean()\n",
    "k = 5\n",
    "for col in ['oem', 'model', 'city']:\n",
    "    target_mean = df.groupby(col)['log_price'].mean()\n",
    "    count = df.groupby(col)['log_price'].count()\n",
    "    smooth = (target_mean * count + mean_global * k) / (count + k)\n",
    "    df[f'{col}_target_enc'] = df[col].map(smooth)\n",
    "for col in ['oem', 'model', 'city']:\n",
    "    freq = df[col].value_counts()\n",
    "    df[f'{col}_freq_enc'] = df[col].map(freq)\n",
    "df['brand_age'] = df['car_age'] * df['oem_target_enc']\n",
    "df['km_per_year_age'] = df['km_per_year'] * df['car_age']\n",
    "df['power_weight_ratio'] = df['max power delivered'] / df['kerb weight']\n",
    "\n",
    "# Define features\n",
    "num_cols = [\n",
    "    'km', 'car_age', 'km_per_year', 'max power delivered', 'alloy wheel size',\n",
    "    'length', 'width', 'height', 'wheel base', 'front tread', 'rear tread',\n",
    "    'kerb weight', 'gross weight', 'top speed', 'acceleration', 'bore',\n",
    "    'oem_target_enc', 'model_target_enc', 'city_target_enc',\n",
    "    'brand_age', 'km_per_year_age', 'power_weight_ratio'\n",
    "]\n",
    "cat_cols = [\n",
    "    'transmission', 'fuel', 'owner_type', 'drive type', 'steering type',\n",
    "    'front brake type', 'rear brake type', 'tyre type'\n",
    "]\n",
    "num_cols = [col for col in num_cols if col in df.columns]\n",
    "cat_cols = [col for col in cat_cols if col in df.columns]\n",
    "\n",
    "# Prepare features\n",
    "X = df[num_cols + cat_cols].copy()\n",
    "for col in cat_cols:\n",
    "    X[col] = X[col].astype('category')\n",
    "X[num_cols] = scaler.transform(X[num_cols])\n",
    "\n",
    "# SHAP for anomalies only\n",
    "anomaly_indices = anomaly_df['original_index'].tolist()\n",
    "X_anomalies = X.loc[anomaly_indices]\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_anomalies)\n",
    "except Exception as e:\n",
    "    st.error(f\"Error initializing SHAP explainer: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "# Filter by rule violations\n",
    "st.subheader(\"Filter Anomalies by Rule Violations\")\n",
    "rule_options = ['All', 'high_price_high_km', 'high_price_old_car']\n",
    "selected_rule = st.selectbox(\"Select Rule Violation\", rule_options)\n",
    "\n",
    "if selected_rule == 'All':\n",
    "    filtered_anomalies = anomaly_df\n",
    "else:\n",
    "    filtered_anomalies = anomaly_df[anomaly_df['rule_violations'].str.contains(selected_rule, case=False)]\n",
    "\n",
    "# Ensure filtered_anomalies is Arrow-compatible\n",
    "filtered_anomalies = filtered_anomalies.copy().reset_index(drop=True)\n",
    "for col in numeric_cols:\n",
    "    if col in filtered_anomalies.columns:\n",
    "        filtered_anomalies[col] = pd.to_numeric(filtered_anomalies[col], errors='coerce').astype('float64').fillna(0)\n",
    "for col in filtered_anomalies.columns:\n",
    "    if col not in numeric_cols + ['original_index']:\n",
    "        filtered_anomalies[col] = filtered_anomalies[col].astype(str).fillna('Unknown')\n",
    "\n",
    "# Debug types (uncomment if needed)\n",
    "# st.write(\"Filtered DataFrame types:\", filtered_anomalies.dtypes)\n",
    "st.write(f\"**Filtered Anomalies**: {len(filtered_anomalies)}\")\n",
    "\n",
    "# Display anomaly table\n",
    "st.subheader(\"Anomaly Details\")\n",
    "display_cols = ['model', 'oem', 'listed_price', 'predicted_price', 'residual', 'km', 'car_age', 'rule_violations']\n",
    "display_df = filtered_anomalies[display_cols].copy()\n",
    "for col in ['listed_price', 'predicted_price', 'residual']:\n",
    "    display_df[col] = display_df[col].apply(lambda x: f'₹{x:,.0f}' if pd.notnull(x) else '₹0')\n",
    "for col in ['km', 'car_age']:\n",
    "    display_df[col] = display_df[col].apply(lambda x: f'{x:,.0f}' if pd.notnull(x) else '0')\n",
    "st.dataframe(display_df)\n",
    "\n",
    "# SHAP explanation\n",
    "st.subheader(\"SHAP Explanation for Selected Anomaly\")\n",
    "if len(filtered_anomalies) > 0:\n",
    "    selected_index = st.selectbox(\"Select Anomaly Index\", filtered_anomalies.index)\n",
    "    original_index = filtered_anomalies.loc[selected_index, 'original_index']\n",
    "    if original_index in X_anomalies.index:\n",
    "        st.write(f\"**Details for Anomaly (Index {selected_index})**\")\n",
    "        anomaly_details = filtered_anomalies.loc[selected_index, display_cols].copy()\n",
    "        anomaly_details_display = pd.DataFrame([anomaly_details])\n",
    "        for col in ['listed_price', 'predicted_price', 'residual']:\n",
    "            anomaly_details_display[col] = anomaly_details_display[col].apply(lambda x: f'₹{x:,.0f}' if pd.notnull(x) else '₹0')\n",
    "        for col in ['km', 'car_age']:\n",
    "            anomaly_details_display[col] = anomaly_details_display[col].apply(lambda x: f'{x:,.0f}' if pd.notnull(x) else '0')\n",
    "        st.dataframe(anomaly_details_display)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        shap_index = anomaly_indices.index(original_index)\n",
    "        shap.force_plot(explainer.expected_value, shap_values[shap_index], X_anomalies.loc[original_index], matplotlib=True, show=False)\n",
    "        plt.title(f'SHAP Force Plot for Anomaly (Index {selected_index})')\n",
    "        st.pyplot(fig)\n",
    "        plt.close()\n",
    "    else:\n",
    "        st.warning(f\"Index {selected_index} not found in dataset.\")\n",
    "else:\n",
    "    st.warning(\"No anomalies available for the selected rule.\")\n",
    "\n",
    "# Update README\n",
    "readme_content = f\"\"\"\n",
    "# Streamlit Demo Skeleton Summary\n",
    "- Built a Streamlit app to display {len(anomaly_df)} validated anomalies from business rules.\n",
    "- Features: Filter by rule violations (high_price_high_km, high_price_old_car), view anomaly details, and display SHAP force plots.\n",
    "- Resolved Arrow serialization issues for DataFrame display.\n",
    "- Next steps: Polish UI with additional visuals and interactive features.\n",
    "\"\"\"\n",
    "with open(os.path.join(PROJECT_ROOT, 'README.md'), 'a', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "st.write('README.md updated with Streamlit demo skeleton summary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c79138-a7aa-45a4-9b82-867ed9840a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
